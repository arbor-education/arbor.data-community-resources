{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0564fc5c-1f2e-4ff4-bf93-e55e1308e76d",
   "metadata": {
    "name": "cell11",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Forecasting School Student Demographic Proportions Over Time"
  },
  {
   "cell_type": "markdown",
   "id": "344f85a0-c426-4961-b308-037e43355f47",
   "metadata": {
    "name": "cell12",
    "collapsed": false,
    "resultHeight": 171
   },
   "source": "### Intro\n\nThis is a notebook demonstrating a simple and flexible forecasting workflow that could be adapted to multiple Machine Learning problems on Arbor BI Connector data model. Using previous 5 years of data I predict the next 3 years of each student demographic proportion.\n\nSee the full data model here: https://support.arbor-education.com/hc/en-us/articles/16774873076637-BI-Connector-Version-2-Datasets#01J58GFDKTPFBE7SK14Z0NQFFQ\n\nIf you have any questions about this then please ask here: https://arbor-hq.circle.so/c/data-and-bi/"
  },
  {
   "cell_type": "markdown",
   "id": "9f849ac0-68c1-42b8-9fab-6fb474f72312",
   "metadata": {
    "name": "cell33",
    "collapsed": false,
    "resultHeight": 1617
   },
   "source": "## Data Science Workflow  \n==================================================  \n**Step: Data Collection**  \n\nDescription: Gathering and preparing data  \nKey Questions:  \n- What data do we have?  \n- Is it clean and reliable?  \n- Do we need more data?  \n\n--------------------------------------------------  \n**Step: Exploratory Data Analysis**\n\nDescription: Understanding data patterns and relationships  \nKey Questions:  \n- What patterns exist in the data?  \n- Are there any obvious correlations?  \n- What are the data distributions?  \n\n--------------------------------------------------  \n**Step: Generate Hypotheses**  \n\nDescription: Forming theories about relationships  \nKey Questions:  \n- What relationships might exist?  \n- Which features could be important?  \n- What are our assumptions?  \n\n--------------------------------------------------  \n**Step: Model/Feature Selection**\n\nDescription: Choosing appropriate models and features  \nKey Questions:  \n- Which models suit our problem?  \n- What features should we use?  \n- What preprocessing is needed?  \n\n--------------------------------------------------  \n\n**Step: Model Building & Tuning**  \nDescription: Creating and optimizing models  \nKey Questions:  \n- How well does the model perform?  \n- What parameters work best?  \n- Are our hypotheses supported?  \n\n\n--------------------------------------------------  \n\n**Step: Error Analysis**  \nDescription: Explore errors for bias or patterns.\n- Analyze prediction errors\n- Look for patterns in mistakes\n- Understand model limitations\n\n--------------------------------------------------  \n\n**Optional:**\nThink about scheduling/ productionising."
  },
  {
   "cell_type": "markdown",
   "id": "0e2a993b-fc1f-4fce-b28b-a08ced1e2255",
   "metadata": {
    "name": "cell8",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Set up Environment"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 0,
    "collapsed": false
   },
   "source": "\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import col, year\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression \nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom prophet import Prophet\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n\n# # We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0df28d-5265-4f00-840f-573017c3ebf1",
   "metadata": {
    "name": "cell7",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Query Training Data From Snowflake"
  },
  {
   "cell_type": "markdown",
   "id": "2a51a9ab-7551-41e3-8877-ad710bb6d05f",
   "metadata": {
    "name": "cell16",
    "collapsed": false,
    "resultHeight": 227
   },
   "source": "For the training data we'll use yearly proportions for each demographic for the past 5 years. As you can see we are querying this data straight from the BI Connector.\n\n\n\nYou can replace 'DEMOGRAPHIC__STUDENT__SEN'  with any of the following to predict any of these demographic proportions:\n\n\n\n**DEMOGRAPHIC__STUDENT__UK_DFE__DISADVANTAGED\nDEMOGRAPHIC__STUDENT__UK_DFE__COMPULSORY_SCHOOL_AGE\nDEMOGRAPHIC__STUDENT__OUT_OF_AGE_GROUP_COHORT\nDEMOGRAPHIC__STUDENT__SEN\nDEMOGRAPHIC__STUDENT__UK_DFE__SERVICE_CHILD\nDEMOGRAPHIC__STUDENT__UK_DFE__PUPIL_PREMIUM\nDEMOGRAPHIC__STUDENT__GIFTED\nDEMOGRAPHIC__STUDENT__IN_YEAR_ADMISSION\nDEMOGRAPHIC__STUDENT__UK_DFE__FSM\nDEMOGRAPHIC__STUDENT__HAS_MEDICAL_CONDITION\nDEMOGRAPHIC__STUDENT__EAL\nDEMOGRAPHIC__STUDENT__UK_DFE__MOBILE_Y10_Y11\nDEMOGRAPHIC__STUDENT__UK_DFE__EVER_6_SERVICE_CHILD\nDEMOGRAPHIC__STUDENT__GIFTED_TALENTED\nDEMOGRAPHIC__STUDENT__TALENTED\nDEMOGRAPHIC__STUDENT__UK_DFE__TRAVELLER\nDEMOGRAPHIC__STUDENT__UK_DFE__EVER_6_FSM\nDEMOGRAPHIC__STUDENT__CHILD_PROTECTION\nDEMOGRAPHIC__STUDENT__IN_CARE\nDEMOGRAPHIC__STUDENT__UK_DFE__MOBILE_Y5_Y6**"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 0
   },
   "source": "# Query Training Data From Snowflake using 'DEMOGRAPHIC__STUDENT__SEN' \n# Notice we use the student_tags_history and academics_year_enrolments to get historical data.\n\nquery = \"\"\"\n    SELECT \n        sa.application_id AS APPLICATION_ID,\n        YEAR(sa.start_date) AS YEAR,\n        COUNT(DISTINCT sa.student_id) AS TOTAL_STUDENTS,\n        COUNT(DISTINCT CASE WHEN st.tag_identifier = 'DEMOGRAPHIC__STUDENT__SEN' \n                            AND (st.end_date IS NULL OR st.end_date >= sa.start_date) \n                            THEN st.student_id END) AS SEN_STUDENTS,\n        COUNT(DISTINCT CASE WHEN st.tag_identifier = 'DEMOGRAPHIC__STUDENT__SEN' \n                            AND (st.end_date IS NULL OR st.end_date >= sa.start_date) \n                            THEN st.student_id END) / \n        NULLIF(COUNT(DISTINCT sa.student_id), 0) AS SEN_PROPORTION\n    FROM \n        ARBOR_BI_CONNECTOR_PRODUCTION.ARBOR_MIS_ENGLAND_MODELLED.STUDENT_ACADEMIC_YEAR_ENROLMENTS AS sa\n    LEFT JOIN \n        ARBOR_BI_CONNECTOR_PRODUCTION.ARBOR_MIS_ENGLAND_MODELLED.STUDENT_TAGS_HISTORY AS st\n    ON \n        sa.student_id = st.student_id \n        AND YEAR(sa.start_date) = YEAR(COALESCE(st.start_date, CURRENT_DATE))\n    WHERE \n        sa.start_date >= DATEADD(year, -5, CURRENT_DATE())\n    GROUP BY \n        sa.application_id, YEAR(sa.start_date)\n    ORDER BY \n        sa.application_id, YEAR;\n\"\"\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5601c552-4488-4780-90a3-ad72647a5841",
   "metadata": {
    "name": "cell26",
    "collapsed": false,
    "resultHeight": 185
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "1d28a031-c664-4f95-ab63-2491fd46d68d",
   "metadata": {
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Data Preprocessing"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "resultHeight": 0,
    "collapsed": false
   },
   "source": "\n# Execute the query and load data into a Snowpark DataFrame\nsp_df = session.sql(query)\n\n# Convert Snowpark DataFrame to Pandas DataFrame for further processing\npandas_df = sp_df.toPandas()\n\n# Check if the DataFrame contains data\nif pandas_df.empty:\n    print(\"The query returned no data.\")\n\n# Sort the DataFrame by 'APPLICATION_ID' and 'YEAR' to maintain a logical order\npandas_df.sort_values(['APPLICATION_ID', 'YEAR'], inplace=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "94dd0d5c-434e-4427-986e-d96697c18904",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "resultHeight": 252,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's look at the training data\npandas_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85fa45c5-5a3c-4bee-b589-398f8b048913",
   "metadata": {
    "name": "cell22",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Create Synthetic Dataset\n"
  },
  {
   "cell_type": "markdown",
   "id": "ccd24844-41ce-48bd-93d9-e6299ff7ed7c",
   "metadata": {
    "name": "cell27",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "An alternative option is to create synthetic data (for example if you want to try out models before using on your own data.) Below is code to create synthetic data in the same structure as BI Connector student demographics."
  },
  {
   "cell_type": "code",
   "id": "32642ab4-900d-43c5-b702-279553e5303a",
   "metadata": {
    "language": "python",
    "name": "cell21",
    "resultHeight": 575,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create synthetic dataset\nnp.random.seed(42)  # For reproducibility - this means the same data will be created every time. Change random seed to change the synthetic data.\n\ndef create_synthetic_data():\n    # Create 10 schools (APPLICATION_ID) with 5 years of data each\n    application_ids = range(1, 11)\n    years = range(2019, 2024)  # 5 years of historical data\n    \n    data = []\n    \n    for app_id in application_ids:\n        # Generate a base SEN proportion (between 1% and 4%)\n        base_proportion = np.random.uniform(0.01, 0.04)\n        \n        # Add some trend and noise\n        trend = np.random.uniform(-0.002, 0.002)  # Small yearly trend\n        \n        for year in years:\n            # Calculate SEN proportion with trend and noise\n            noise = np.random.normal(0, 0.001)  # Small random variation\n            sen_proportion = base_proportion + trend * (year - 2019) + noise\n            \n            # Ensure proportion stays within reasonable bounds\n            sen_proportion = np.clip(sen_proportion, 0, 1)\n            \n            data.append({\n                'APPLICATION_ID': app_id,\n                'YEAR': year,\n                'SEN_PROPORTION': sen_proportion\n            })\n    \n    # Convert to DataFrame\n    pandas_df = pd.DataFrame(data)\n    return pandas_df\n\n# Create synthetic data\nsynthetic_df = create_synthetic_data()\n\n# Display first few rows and summary statistics\nprint(\"First few rows of synthetic data:\")\nprint(synthetic_df.head(10))\n\nprint(\"\\nSummary statistics:\")\nprint(synthetic_df.describe())\n\nprint(\"\\nUnique schools (APPLICATION_IDs):\", len(synthetic_df['APPLICATION_ID'].unique()))\nprint(\"Years per school:\", len(synthetic_df['YEAR'].unique()))\nprint(\"Year range:\", synthetic_df['YEAR'].min(), \"to\", synthetic_df['YEAR'].max())\n\n# Now let's run our models on this synthetic data\n# [Previous model code here - do you want me to include the full modeling code as well?]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d3dbc10-f22d-480a-a25e-890a3b5b1025",
   "metadata": {
    "language": "python",
    "name": "cell28",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Choose whether you want to use synthetic data or data from the BI Connector.\n# If you want to use synthetic data then remove the '#' from the line below: data_to_use = 'synthetic'\n\ndata_to_use = 'bi_connector'\n# data_to_use = 'synthetic'\n\nif data_to_use == 'bi_connector':\n    demographics_df = pandas_df\nelse:\n    demographics_df = synthetic_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e39230c-6e2c-4982-8b5f-61373ff15f2b",
   "metadata": {
    "language": "python",
    "name": "cell29",
    "resultHeight": 252
   },
   "outputs": [],
   "source": "demographics_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38952af1-9ebd-4027-9bcd-48556f6ab24f",
   "metadata": {
    "name": "cell24",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Exploratory Data Analysis"
  },
  {
   "cell_type": "markdown",
   "id": "dfe86b6c-c46e-4807-bee1-90129ada150d",
   "metadata": {
    "name": "cell25",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Now we'll do some exploratory data analysis to understand the training data we will be using."
  },
  {
   "cell_type": "code",
   "id": "40ab9324-a390-4a47-999c-f3699297f3a2",
   "metadata": {
    "language": "python",
    "name": "cell23",
    "resultHeight": 1668
   },
   "outputs": [],
   "source": "# Create a figure with multiple subplots\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Distribution of SEN Proportions\nsns.histplot(data=demographics_df, x='SEN_PROPORTION', bins=20, ax=ax1)\nax1.set_title('Distribution of SEN Proportions', fontsize=12)\nax1.set_xlabel('SEN Proportion')\nax1.set_ylabel('Count')\nax1.grid(True, linestyle='--', alpha=0.7)\n\n# 2. Box plot of SEN Proportions by Year\nsns.boxplot(data=demographics_df, x='YEAR', y='SEN_PROPORTION', ax=ax2)\nax2.set_title('SEN Proportions by Year', fontsize=12)\nax2.set_xlabel('Year')\nax2.set_ylabel('SEN Proportion')\nax2.grid(True, linestyle='--', alpha=0.7)\n\n# 3. Line plot showing trends for each school\nfor app_id in demographics_df['APPLICATION_ID'].unique():\n    school_data = demographics_df[demographics_df['APPLICATION_ID'] == app_id]\n    ax3.plot(school_data['YEAR'], school_data['SEN_PROPORTION'], \n            marker='o', label=f'School {app_id}')\nax3.set_title('SEN Proportion Trends by School', fontsize=12)\nax3.set_xlabel('Year')\nax3.set_ylabel('SEN Proportion')\nax3.grid(True, linestyle='--', alpha=0.7)\nax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n\n# 4. Box plot of SEN Proportions by School\nsns.boxplot(data=demographics_df, x='APPLICATION_ID', y='SEN_PROPORTION', ax=ax4)\nax4.set_title('SEN Proportions by School', fontsize=12)\nax4.set_xlabel('School ID')\nax4.set_ylabel('SEN Proportion')\nax4.grid(True, linestyle='--', alpha=0.7)\n\nplt.tight_layout()\n# plt.savefig('demographics_data_eda.png', bbox_inches='tight', dpi=300)\n# plt.close()\n\n# Print summary statistics\nprint(\"\\nSummary Statistics:\")\nprint(demographics_df.groupby('YEAR')['SEN_PROPORTION'].describe().round(4))\n\n# Print trend analysis\nprint(\"\\nTrend Analysis:\")\nfor app_id in demographics_df['APPLICATION_ID'].unique():\n    school_data = demographics_df[demographics_df['APPLICATION_ID'] == app_id]\n    start_val = school_data.iloc[0]['SEN_PROPORTION']\n    end_val = school_data.iloc[-1]['SEN_PROPORTION']\n    change = end_val - start_val\n    print(f\"School {app_id}: Overall change = {change:.4f} ({'+' if change > 0 else ''}{change/start_val*100:.1f}%)\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7543612b-5c70-44b4-85d7-b1dc0960da02",
   "metadata": {
    "name": "cell38",
    "collapsed": false,
    "resultHeight": 332
   },
   "source": "**Analysing our Data**\n\nBased on our Exploratory data analysis we identify the features (columns) to use to predict our outcome variable and choose which models might be best to use. \n\n\nHere is a checklist for what to look for in time series data:\n\n**Trend Analysis:**\nLook for a clear, linear trend in the data over time.\nUse line plots to visualize the relationship between time and your target variable.\n\n**Seasonality:**\nMinimal or no repeating seasonal patterns.\n\n**Stationarity:**\nData doesn't have sudden shifts or trends that vary over time.\n\n**Variance:**\nVariance remains relatively constant over time.\n\nIdeal Use Case:\nThe data has a linear trend and lacks seasonality or irregular fluctuations.\nExamples: Steady growth/decline in student demographic proportions.\n"
  },
  {
   "cell_type": "markdown",
   "id": "31c47f01-4996-4507-9fc4-74a235f0287b",
   "metadata": {
    "name": "cell39",
    "collapsed": false,
    "resultHeight": 217
   },
   "source": "**Choosing a Model**\n\nBased on understanding our data we can then choose a model. \n\nFor Linear Regression, use it when the data shows a clear linear trend with no seasonality or complex patterns; it's ideal for straightforward, steady trends over time. SARIMA is better for data with clear seasonality and autocorrelation, requiring stationarity (check with ADF and ACF/PACF). Prophet handles non-linear trends and flexible seasonality, robust to outliers and missing data. Exponential Smoothing (Holt-Winters) is best for simpler data with consistent trend and seasonality. Machine Learning models excel at capturing complex, non-linear patterns or multivariate relationships. Choose based on your data's trends, seasonality, and complexity.\n\nFor this task we will use linear regression (as most demographic proportions trends are likely to be linear without complex patterns), SARIMA (to demonstrate that this model might not be best suited for this task) and as a benchmark we use a simple average as our third model."
  },
  {
   "cell_type": "markdown",
   "id": "e3dba050-a985-46e6-8611-f49689e4e4ce",
   "metadata": {
    "name": "cell10",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Define Models"
  },
  {
   "cell_type": "code",
   "id": "72a20799-899a-498e-9800-0a463d1e4d2f",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "def train_sarima(app_df):\n    \"\"\"Train SARIMA model and return predictions and MAE\"\"\"\n    try:\n        model = SARIMAX(app_df['SEN_PROPORTION'], \n                       order=(1, 1, 1), \n                       seasonal_order=(0, 0, 0, 0),\n                       enforce_stationarity=False,\n                       enforce_invertibility=False)\n        result = model.fit(disp=0)\n        forecast = result.get_forecast(steps=3)\n        predictions = forecast.predicted_mean.values  # Convert to numpy array\n        mae = mean_absolute_error(app_df['SEN_PROPORTION'], result.fittedvalues)\n        \n        # Add validation check\n        if np.any(predictions < 0) or np.any(predictions > 1):\n            return None, None\n        return predictions, mae\n    except Exception as e:\n        print(f\"SARIMA error: {e}\")\n        return None, None\n\ndef train_simple_average(app_df):\n    \"\"\"Calculate simple moving average\"\"\"\n    try:\n        mean_value = app_df['SEN_PROPORTION'].mean()\n        predictions = np.array([mean_value] * 3)\n        mae = mean_absolute_error(app_df['SEN_PROPORTION'], \n                                np.array([mean_value] * len(app_df)))\n        return predictions, mae\n    except Exception as e:\n        print(f\"Simple Average error: {e}\")\n        return None, None\n\ndef train_linear_regression(app_df):\n    \"\"\"Simple linear regression projection\"\"\"\n    try:\n        # Need at least 2 points for regression\n        if len(app_df) < 2:\n            return None, None\n            \n        X = np.arange(len(app_df)).reshape(-1, 1)\n        y = app_df['SEN_PROPORTION'].values\n        \n        # Check if there's any variation in the data\n        if np.all(y == y[0]):\n            return np.array([y[0]] * 3), 0\n        \n        model = LinearRegression()\n        model.fit(X, y)\n        \n        # Project next three years - each year should be different following the trend\n        future_X = np.arange(len(app_df), len(app_df) + 3).reshape(-1, 1)\n        predictions = model.predict(future_X)\n        \n        # Print debugging info\n        print(f\"\\nDebug for years {app_df.index[0]} to {app_df.index[-1]}:\")\n        print(f\"Historical values: {y}\")\n        print(f\"Slope (change per year): {model.coef_[0]:.6f}\")\n        print(f\"Intercept: {model.intercept_:.6f}\")\n        print(f\"Predicted next 3 years: {predictions}\")\n        \n        # Ensure predictions stay within reasonable bounds\n        min_allowed = max(0, min(y) * 0.5)  # Don't go below 0\n        max_allowed = min(1, max(y) * 1.5)  # Don't go above 1 (100%)\n        predictions = np.clip(predictions, min_allowed, max_allowed)\n        \n        # Calculate MAE on training data\n        train_predictions = model.predict(X)\n        mae = mean_absolute_error(y, train_predictions)\n        \n        # Validate predictions\n        if np.all(predictions <= 0) or np.all(predictions >= 1):\n            print(\"Warning: Predictions out of reasonable range\")\n            return None, None\n            \n        return predictions, mae\n        \n    except Exception as e:\n        print(f\"Linear Regression error: {e}\")\n        return None, None\n\ndef compare_models(pandas_df):\n    \"\"\"Compare different forecasting models and return predictions\"\"\"\n    predictions_list = []\n    total_apps = len(pandas_df['APPLICATION_ID'].unique())\n    \n    for idx, app_id in enumerate(pandas_df['APPLICATION_ID'].unique(), 1):\n        print(f\"\\rProcessing application {idx}/{total_apps}\", end=\"\")\n        \n        # Filter for current application\n        app_df = pandas_df[pandas_df['APPLICATION_ID'] == app_id].copy()\n        app_df.set_index('YEAR', inplace=True)\n        \n        # Dictionary to store model results\n        model_results = {\n            'SARIMA': train_sarima(app_df),\n            'Simple Average': train_simple_average(app_df),\n            'Linear Regression': train_linear_regression(app_df)\n        }\n        \n        # Store predictions for each valid model\n        for model_name, (predictions, mae) in model_results.items():\n            if predictions is not None and mae is not None:\n                # Additional validation check\n                if np.all((predictions >= 0) & (predictions <= 1)):\n                    forecast_index = [app_df.index[-1] + i for i in range(1, 4)]\n                    for year, prediction in zip(forecast_index, predictions):\n                        predictions_list.append({\n                            'APPLICATION_ID': app_id,\n                            'YEAR': int(year),\n                            'PREDICTED_SEN_PROPORTION': float(prediction),\n                            'MAE': float(mae),\n                            'MODEL_USED': model_name\n                        })\n    \n    print(\"\\nProcessing complete!\")\n    return pd.DataFrame(predictions_list)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b9e86e9-9045-4b84-b7c8-265f38c1cf5d",
   "metadata": {
    "name": "cell14",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Results"
  },
  {
   "cell_type": "code",
   "id": "7323cd8a-b221-4d7a-b9fc-7ca8ea8917d8",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "resultHeight": 1895
   },
   "outputs": [],
   "source": "# Run the comparison\npredictions_df = compare_models(demographics_df)\n\n# Display summary of model performance\nprint(\"\\nModel Usage Summary:\")\nmodel_counts = predictions_df['MODEL_USED'].value_counts()\nprint(model_counts)\n\nprint(\"\\nAverage MAE by Model:\")\navg_mae = predictions_df.groupby('MODEL_USED')['MAE'].mean()\nprint(avg_mae)\n\n# Add some basic validation checks\nprint(\"\\nPrediction Range Check:\")\nprint(\"Min predicted value:\", predictions_df['PREDICTED_SEN_PROPORTION'].min())\nprint(\"Max predicted value:\", predictions_df['PREDICTED_SEN_PROPORTION'].max())\n\n# # Save results to CSV\n# predictions_df.to_csv('sen_predictions_comparison.csv', index=False)\n# print(\"\\nResults saved to 'sen_predictions_comparison.csv'\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b77cfc3d-aabf-490b-8809-fb84fc5e11c7",
   "metadata": {
    "name": "cell30",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "We are using Mean Absolute Error (MAE) as the metric to measure quality of model.This is quite standard for regression tasks, although Mean Squared Error (MSE) could also be used."
  },
  {
   "cell_type": "code",
   "id": "c197b70f-cb81-4b73-8eea-9f1d30c3b205",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "resultHeight": 479
   },
   "outputs": [],
   "source": "\n# Create figure\nplt.figure(figsize=(15, 10))\n\n# Create subplot grid\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# 1. Boxplot of MAEs by model\nsns.boxplot(data=predictions_df, x='MODEL_USED', y='MAE', ax=ax1)\nax1.set_title('Distribution of MAE by Model', fontsize=12, pad=20)\nax1.set_xlabel('Model', fontsize=10)\nax1.set_ylabel('Mean Absolute Error', fontsize=10)\nax1.tick_params(axis='x', rotation=45)\nax1.grid(True, linestyle='--', alpha=0.7)\n\n# Add individual points\nsns.swarmplot(data=predictions_df, x='MODEL_USED', y='MAE', \n             color='red', alpha=0.5, ax=ax1)\n\n# 2. Bar plot with error bars\nmae_stats = predictions_df.groupby('MODEL_USED').agg({\n    'MAE': ['mean', 'std', 'min', 'max']\n})['MAE']\n\n# Plot bars\nx = range(len(mae_stats.index))\nbars = ax2.bar(x, mae_stats['mean'], \n              yerr=mae_stats['std'],\n              capsize=5,\n              color=['lightblue', 'lightgreen', 'lightpink'])\n\n# Add value labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f'{height:.4f}',\n            ha='center', va='bottom')\n\nax2.set_xticks(x)\nax2.set_xticklabels(mae_stats.index, rotation=45)\nax2.set_title('Average MAE by Model (with Standard Deviation)', fontsize=12, pad=20)\nax2.set_xlabel('Model', fontsize=10)\nax2.set_ylabel('Mean Absolute Error', fontsize=10)\nax2.grid(True, linestyle='--', alpha=0.7)\n\n# Adjust layout\nplt.tight_layout()\n\n# # Save plot\n# plt.savefig('mae_comparison.png', bbox_inches='tight', dpi=300)\n# plt.close()\n\n# Print detailed statistics\nprint(\"\\nDetailed MAE Statistics:\")\nprint(mae_stats.round(6))\n\n# Add statistical significance test\nfrom scipy import stats\n\n# Perform Kruskal-Wallis H-test\nmodels = predictions_df['MODEL_USED'].unique()\nmae_data = [predictions_df[predictions_df['MODEL_USED'] == model]['MAE'] for model in models]\nh_stat, p_value = stats.kruskal(*mae_data)\n\nprint(\"\\nStatistical Test Results:\")\nprint(f\"Kruskal-Wallis H-test:\")\nprint(f\"H-statistic: {h_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Pairwise Mann-Whitney U tests if Kruskal-Wallis is significant\nif p_value < 0.05:\n    print(\"\\nPairwise Model Comparisons (Mann-Whitney U test):\")\n    for i, model1 in enumerate(models):\n        for model2 in models[i+1:]:\n            stat, p = stats.mannwhitneyu(\n                predictions_df[predictions_df['MODEL_USED'] == model1]['MAE'],\n                predictions_df[predictions_df['MODEL_USED'] == model2]['MAE']\n            )\n            print(f\"{model1} vs {model2}:\")\n            print(f\"p-value: {p:.4f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fecaf329-73d3-4f73-a848-6a78c8c98346",
   "metadata": {
    "name": "cell6",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Full Predictions"
  },
  {
   "cell_type": "code",
   "id": "0f66b878-d3d5-4861-bb47-2566fc1f6169",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "resultHeight": 439
   },
   "outputs": [],
   "source": "predictions_df.head(25)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf03fc3f-b7b8-43d5-9169-8e606e38c34a",
   "metadata": {
    "name": "cell19",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Visualise Predictions"
  },
  {
   "cell_type": "markdown",
   "id": "7b3caf8f-93b8-4d46-95ea-4f5591705e81",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "e1e81d03-079c-45f4-8f1d-1fba43942090",
   "metadata": {
    "language": "python",
    "name": "cell20",
    "resultHeight": 457
   },
   "outputs": [],
   "source": "\n# Create figure with multiple subplots\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 20))\n\n# 1. Box plot of predictions by model\nmodels = predictions_df['MODEL_USED'].unique()\ndata = [predictions_df[predictions_df['MODEL_USED'] == model]['PREDICTED_SEN_PROPORTION'] \n        for model in models]\n\nbox_plot = ax1.boxplot(data, labels=models, patch_artist=True)\n# Color each box\ncolors = ['lightblue', 'lightgreen', 'lightpink']\nfor patch, color in zip(box_plot['boxes'], colors):\n    patch.set_facecolor(color)\n\nax1.set_title('Distribution of Predictions by Model', fontsize=12, pad=20)\nax1.set_xlabel('Model', fontsize=10)\nax1.set_ylabel('Predicted SEN Proportion', fontsize=10)\nax1.tick_params(axis='x', rotation=45)\nax1.grid(True, linestyle='--', alpha=0.7)\n\n# 2. Line plot for each school\n# Create a color map for schools\nunique_schools = demographics_df['APPLICATION_ID'].unique()\ncolors = plt.cm.tab20(np.linspace(0, 1, len(unique_schools)))\n\nfor i, app_id in enumerate(unique_schools):\n    # Historical data\n    hist_data = demographics_df[demographics_df['APPLICATION_ID'] == app_id]\n    ax2.plot(hist_data['YEAR'], hist_data['SEN_PROPORTION'], \n            marker='o', linestyle='-', color=colors[i], alpha=0.6,\n            label=f'School {app_id} - Historical')\n    \n    # Predictions for each model\n    pred_data = predictions_df[predictions_df['APPLICATION_ID'] == app_id]\n    for model in pred_data['MODEL_USED'].unique():\n        model_pred = pred_data[pred_data['MODEL_USED'] == model]\n        ax2.plot(model_pred['YEAR'], model_pred['PREDICTED_SEN_PROPORTION'], \n                marker='s', linestyle='--', color=colors[i], alpha=0.4)\n\nax2.set_title('Historical Values and Predictions by School', fontsize=12, pad=20)\nax2.set_xlabel('Year', fontsize=10)\nax2.set_ylabel('SEN Proportion', fontsize=10)\nax2.grid(True, linestyle='--', alpha=0.7)\n\n# Add legend with smaller font and to the right\nax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n\n# 3. MAE comparison\nmae_by_model = predictions_df.groupby('MODEL_USED')['MAE'].mean()\nx = np.arange(len(mae_by_model))\nbars = ax3.bar(x, mae_by_model.values, color=['lightblue', 'lightgreen', 'lightpink'])\n\n# Add value labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n            f'{height:.4f}',\n            ha='center', va='bottom')\n\nax3.set_xticks(x)\nax3.set_xticklabels(mae_by_model.index, rotation=45)\nax3.set_title('Mean Absolute Error by Model', fontsize=12, pad=20)\nax3.set_xlabel('Model', fontsize=10)\nax3.set_ylabel('Mean Absolute Error', fontsize=10)\nax3.grid(True, linestyle='--', alpha=0.7)\n\n# Adjust layout\nplt.tight_layout()\n\n# Save main plot\nplt.savefig('sen_predictions_visualization.png', bbox_inches='tight', dpi=300)\nplt.close()\n\n# Create individual plots for each school\nfor app_id in unique_schools:\n    plt.figure(figsize=(12, 6))\n    \n    # Historical data\n    hist_data = demographics_df[demographics_df['APPLICATION_ID'] == app_id]\n    plt.plot(hist_data['YEAR'], hist_data['SEN_PROPORTION'], \n            marker='o', linestyle='-', label='Historical', color='black', linewidth=2)\n    \n    # Predictions for each model\n    pred_data = predictions_df[predictions_df['APPLICATION_ID'] == app_id]\n    colors = ['blue', 'green', 'red']  # Different color for each model\n    for i, model in enumerate(pred_data['MODEL_USED'].unique()):\n        model_pred = pred_data[pred_data['MODEL_USED'] == model]\n        plt.plot(model_pred['YEAR'], model_pred['PREDICTED_SEN_PROPORTION'], \n                marker='s', linestyle='--', label=f'{model}',\n                color=colors[i], linewidth=2)\n    \n    plt.title(f'School {app_id}: Historical and Predicted SEN Proportions', fontsize=12, pad=20)\n    plt.xlabel('Year', fontsize=10)\n    plt.ylabel('SEN Proportion', fontsize=10)\n    plt.legend(fontsize=9)\n    plt.grid(True, linestyle='--', alpha=0.7)\n    \n    # Save individual plot\n    # plt.savefig(f'school_{app_id}_predictions.png', bbox_inches='tight', dpi=300)\n    # plt.close()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c8fb1fe-f260-475e-8a76-1f3e6db12bac",
   "metadata": {
    "name": "cell34",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Error Analysis"
  },
  {
   "cell_type": "markdown",
   "id": "be1caea6-be18-497c-928c-b40809610d6b",
   "metadata": {
    "name": "cell36",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Now we can look more in-depth at the kinds of errors our model is making."
  },
  {
   "cell_type": "code",
   "id": "8615a690-fc14-415d-9d5a-8b430531ece3",
   "metadata": {
    "language": "python",
    "name": "cell37",
    "resultHeight": 1326
   },
   "outputs": [],
   "source": "# Create figure with 2 key plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# 1. Distribution of predictions by model\nsns.boxplot(data=predictions_df, x='MODEL_USED', y='PREDICTED_SEN_PROPORTION', ax=ax1)\nax1.set_title('Distribution of Predictions by Model')\nax1.set_xlabel('Model')\nax1.set_ylabel('Predicted SEN Proportion')\nax1.tick_params(axis='x', rotation=45)\nax1.grid(True, linestyle='--', alpha=0.7)\n\n# 2. Predictions over time by model\nsns.lineplot(data=predictions_df, x='YEAR', y='PREDICTED_SEN_PROPORTION', \n            hue='MODEL_USED', marker='o', ax=ax2)\nax2.set_title('Predictions Over Time by Model')\nax2.set_xlabel('Year')\nax2.set_ylabel('Predicted SEN Proportion')\nax2.grid(True, linestyle='--', alpha=0.7)\n\nplt.tight_layout()\n\n# Print basic statistics\nprint(\"\\nSummary Statistics by Model:\")\nstats = predictions_df.groupby('MODEL_USED')['PREDICTED_SEN_PROPORTION'].agg([\n    'count', 'mean', 'std', 'min', 'max'\n]).round(4)\nprint(stats)\n\n# Print predictions range for each year\nprint(\"\\nPrediction Ranges by Year:\")\nyearly_stats = predictions_df.groupby(['YEAR', 'MODEL_USED'])['PREDICTED_SEN_PROPORTION'].agg([\n    'mean', 'min', 'max'\n]).round(4)\nprint(yearly_stats)\n\n# Compare MAE between models\nprint(\"\\nMAE Comparison:\")\nmae_stats = predictions_df.groupby('MODEL_USED')['MAE'].agg([\n    'mean', 'std', 'min', 'max'\n]).round(4)\nprint(mae_stats)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb15d15c-6253-487b-b3cc-a4580ce29d8c",
   "metadata": {
    "name": "cell40",
    "collapsed": false,
    "resultHeight": 732
   },
   "source": "We can now analyse this error analysis:\n\nDistribution of Predictions by Model (Left Plot):\n\n\nSARIMA shows the widest spread, with some outlier predictions as high as 0.8 (80% SEN proportion) which seems unrealistic\nSimple Average shows a very tight distribution around a low value\nLinear Regression also shows a concentrated distribution near the lower end\nMost predictions across all models are clustered in the lower range (likely below 0.1 or 10% SEN)\n\n\nPredictions Over Time by Model (Right Plot):\n\n\nSARIMA (blue line) shows concerning behavior:\n\nIt has increasing uncertainty over time (widening blue shaded area)\nMakes unrealistic predictions going up to 0.5 (50% SEN) by 2027\nShows high volatility in predictions\n\n\nSimple Average (orange line) remains stable over time\n\nThis makes sense as it's using the historical mean\nAppears to predict around 2-3% SEN consistently\n\n\nLinear Regression (green line) stays relatively flat\n\nPredictions appear more conservative and realistic\nSimilar range to Simple Average\n\n\n\nKey Issues and Recommendations:\n\nSARIMA appears unstable and likely inappropriate for this data:\n\nThe increasing uncertainty and extreme predictions suggest it's not handling the time series well\nConsider removing or recalibrating the SARIMA model\n\n\nSimple Average and Linear Regression seem more reliable:\n\nTheir predictions stay within realistic ranges for SEN proportions\nThey show more stable behavior over time\n\n\nModel Selection:\n\nGiven this analysis, I would lean towards using Linear Regression\nSARIMA should probably be discarded unless it can be significantly improved"
  },
  {
   "cell_type": "markdown",
   "id": "b961ec22-cc78-42ba-924a-a6de19d62627",
   "metadata": {
    "name": "cell32",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Next Steps and Learning Resources"
  },
  {
   "cell_type": "markdown",
   "id": "c2e72fbe-f88b-4101-927c-035f714dc3a1",
   "metadata": {
    "name": "cell35",
    "collapsed": false,
    "resultHeight": 111
   },
   "source": "Here are some resources to learn more about time series forecasting:\n- https://www.kaggle.com/learn/time-series\n- https://quickstarts.snowflake.com/guide/getting-started-with-snowflake-cortex-ml-forecasting-and-classification/index.html#0"
  },
  {
   "cell_type": "markdown",
   "id": "53c7cf29-a5fa-4173-b36c-79a50c40f9d5",
   "metadata": {
    "name": "cell31",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "If you have any questions about this notebook please ask them here: https://arbor-hq.circle.so/c/data-and-bi/"
  }
 ]
}